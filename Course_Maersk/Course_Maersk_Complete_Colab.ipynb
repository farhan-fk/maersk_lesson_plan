{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# OpenAI Course - Maersk Edition (Complete)\n",
    "## Comprehensive Guide to OpenAI's API with Maersk Logistics Examples\n",
    "\n",
    "This notebook covers:\n",
    "1. Basic Prompting & API Connection\n",
    "2. Advanced Prompting Techniques\n",
    "3. Chain of Thought & Self-Evaluation\n",
    "4. Chat Applications (Simple & Memory)\n",
    "5. Function Calling & Structured Outputs\n",
    "6. OpenAI's New Features (GPT-5, Audio, Image, Reasoning)\n",
    "7. Tool Use & RAG (Retrieval Augmented Generation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup: API Key Configuration\n",
    "\n",
    "**Choose ONE of the following methods:**\n",
    "\n",
    "### Method 1: Hardcoded API Key (Quick Start)\n",
    "Directly set your API key in the code (not recommended for production)\n",
    "\n",
    "### Method 2: Environment Variable (Recommended)\n",
    "Use Google Colab's Secrets feature or a .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api_key_setup"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# =====================================================\n",
    "# METHOD 1: HARDCODED API KEY (Uncomment to use)\n",
    "# =====================================================\n",
    "# OPENAI_API_KEY = \"sk-your-api-key-here\"  # Replace with your actual API key\n",
    "# client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# =====================================================\n",
    "# METHOD 2: ENVIRONMENT VARIABLE (Recommended)\n",
    "# =====================================================\n",
    "# For Google Colab: Use Secrets (Key icon on left sidebar)\n",
    "# Add a secret named: OPENAI_API_KEY\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    print(\"‚úÖ API Key loaded from Colab Secrets\")\n",
    "except:\n",
    "    # Fallback: Try environment variable or hardcoded\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'YOUR_API_KEY_HERE')\n",
    "    if OPENAI_API_KEY == 'YOUR_API_KEY_HERE':\n",
    "        print(\"‚ö†Ô∏è WARNING: Please set your API key above in METHOD 1 or use Colab Secrets\")\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    print(\"‚úÖ API Key loaded from environment or hardcoded value\")\n",
    "\n",
    "print(\"\\nüéØ OpenAI Client Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lesson0"
   },
   "source": [
    "---\n",
    "# Lesson 0: Basic Connection & Simple Prompting\n",
    "\n",
    "Learn how to:\n",
    "- Connect to OpenAI API\n",
    "- Make basic requests\n",
    "- Use prompt interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lesson0_basic"
   },
   "outputs": [],
   "source": [
    "# Basic API call\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=\"Tell me Maersk company inception story in 200 words max\"\n",
    ")\n",
    "\n",
    "print(\"üìñ Maersk Story:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lesson0_interpolation"
   },
   "outputs": [],
   "source": [
    "# Prompt Interpolation Example\n",
    "religion = \"Hindu\"\n",
    "country = \"India\"\n",
    "gender = \"girl\"\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Suggest five beautiful {gender} baby names.\n",
    "The names should be popular in {country} and follow {religion} traditions.\n",
    "Return only the names as a list.\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"üë∂ Baby Name Suggestions:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lesson1"
   },
   "source": [
    "---\n",
    "# Lesson 1: Advanced Prompting Techniques\n",
    "\n",
    "**Key Principles:**\n",
    "1. Write clear and specific instructions\n",
    "2. Use delimiters\n",
    "3. Ask for structured output (JSON)\n",
    "4. Check conditions\n",
    "5. Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lesson1_delimiters"
   },
   "outputs": [],
   "source": [
    "# Tactic 1: Use delimiters\n",
    "text = \"\"\"\n",
    "You should express what you want a model to do by \\\n",
    "providing instructions that are as clear and \\\n",
    "specific as you can possibly make them. \\\n",
    "This will guide the model towards the desired output, \\\n",
    "and reduce the chances of receiving irrelevant \\\n",
    "or incorrect responses.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\\n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"üìù Summary:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lesson1_json"
   },
   "outputs": [],
   "source": [
    "# Tactic 2: Ask for structured output (JSON)\n",
    "prompt = \"\"\"\n",
    "Generate a list of three made-up spare parts for container ships along \\\n",
    "with their manufacturers and primary use cases.\n",
    "Provide them in JSON format with the following keys:\n",
    "part_id, name, manufacturer, use_case.\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"üì¶ Spare Parts (JSON):\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lesson1_conditions"
   },
   "outputs": [],
   "source": [
    "# Tactic 3: Check conditions\n",
    "inventory = \"\"\"\n",
    "{\n",
    "  \"parts\": [\n",
    "    {\"part_id\": \"SP-ENG-001\", \"name\": \"Marine Diesel Filter\", \"stock\": {\"Mumbai_Warehouse\": 45, \"Chennai_Depot\": 12}},\n",
    "    {\"part_id\": \"SP-DEC-002\", \"name\": \"Hydraulic Pump\", \"stock\": {\"Mumbai_Warehouse\": 0, \"Chennai_Depot\": 8}},\n",
    "    {\"part_id\": \"SP-NAV-003\",  \"name\": \"Radar Antenna\",   \"stock\": {\"Mumbai_Warehouse\": 0, \"Chennai_Depot\": 0}}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "request_text = \"\"\"\n",
    "Ship engineer needs part_id=SP-DEC-002 at Mumbai Port.\n",
    "Preferred pickup location: Mumbai_Warehouse.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with:\n",
    "1) A JSON inventory database for ship spare parts\n",
    "2) A service request\n",
    "\n",
    "Task:\n",
    "- If the requested part is available (stock > 0) at the preferred location, output steps for pickup.\n",
    "- If not available there but available at another location, output steps for transfer.\n",
    "- If not available anywhere, output \"No stock available\".\n",
    "\n",
    "Return your result in the format:\n",
    "Stock Status: <Available/Not Available>\n",
    "Location: <Pickup/Transfer/NA>\n",
    "\n",
    "--- INVENTORY JSON ---\n",
    "{inventory}\n",
    "\n",
    "--- SERVICE REQUEST ---\n",
    "{request_text}\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"üîç Inventory Check:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lesson1_fewshot"
   },
   "outputs": [],
   "source": [
    "# Tactic 4: Few-shot prompting\n",
    "text = \"Write email on container delay issue.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Your task is to write email in a consistent writing style.\n",
    "The style is short, message-like emails (imperfect English, direct tone).\n",
    "\n",
    "<email>:\n",
    "hi sir, container stuck at customs mumbai, we submit docs last week but clearance still pending,\n",
    "pls check urgent.\n",
    "\n",
    "<email>:\n",
    "sir, customer waiting since 3 days, shipment not yet departed from chennai,\n",
    "vessel schedule changed, pls arrange fast tracking.\n",
    "\n",
    "<email>:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"üìß Generated Email:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lesson3"
   },
   "source": [
    "---\n",
    "# Lesson 3: Chain of Thought & Self-Evaluation\n",
    "\n",
    "Give the model time to think:\n",
    "- Break down complex tasks step-by-step\n",
    "- Ask model to verify its own work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lesson3_cot"
   },
   "outputs": [],
   "source": [
    "# Chain of Thought: Analyzing customer service call\n",
    "call_transcript = \"\"\"\n",
    "Agent: Thank you for calling Maersk Logistics, this is Priya. How may I help you today?\n",
    "Customer: I'm honestly very frustrated! My container was supposed to arrive three days ago, and I still don't have any updates!\n",
    "Agent: I'm sorry to hear that, sir. Could you provide me the container number so I can check the status?\n",
    "Customer: It's MAEU7894561. I've been calling every day, and no one gives me a clear answer!\n",
    "Agent: I understand how frustrating that must be. Let me check your shipment details‚Ä¶ yes, I can see the container is currently held at Nhava Sheva port awaiting customs clearance.\n",
    "Customer: Customs clearance? Why wasn't I informed about this? My production line is stopped because of this delay!\n",
    "Agent: I apologize for the lack of communication. It appears there was a documentation issue that's causing the delay.\n",
    "Customer: Documentation issue? We submitted everything correctly! Are you people even checking properly or just making excuses?\n",
    "Agent: I assure you, sir, we take every case seriously. I can see that our customs team is working to resolve this, but it may take 2-3 more business days.\n",
    "Customer: 2-3 more days?! That's unacceptable! I'm losing thousands of rupees every day. I need this resolved immediately!\n",
    "Agent: I completely understand your concern. Unfortunately, customs procedures are beyond our direct control, but I can escalate this to our port operations manager.\n",
    "Customer: Escalate it then! This is ridiculous. I pay premium rates for your services and this is what I get?\n",
    "Agent: I will escalate your case right away, sir. Would you like me to transfer you to my supervisor immediately?\n",
    "Customer: Yes, do it now. I want to speak to someone who can actually help me, not just give me excuses.\n",
    "Agent: Understood, sir. I'll transfer your call to my supervisor immediately. Please hold.\n",
    "\"\"\"\n",
    "\n",
    "enterprise_prompt = f\"\"\"\n",
    "You are analyzing a customer service call transcript. Follow these steps in order to produce a structured JSON report.\n",
    "\n",
    "<transcript>\n",
    "{call_transcript}\n",
    "</transcript>\n",
    "\n",
    "## Analysis Steps\n",
    "\n",
    "**Step 1: Validate Data Sufficiency**\n",
    "First, determine if the transcript contains enough information for analysis.\n",
    "\n",
    "**Step 2: Analyze Sentiment & Tone**\n",
    "Evaluate the emotional aspects of the interaction.\n",
    "\n",
    "**Step 3: Classify the Interaction**\n",
    "Identify interaction type, urgency, and whether escalation is needed.\n",
    "\n",
    "**Step 4: Summarize Key Information**\n",
    "Capture the core issue and resolution.\n",
    "\n",
    "## Output Requirements\n",
    "Return ONLY valid JSON with:\n",
    "- sentiment (overall, customerEmotion, agentTone)\n",
    "- classification (interactionType, urgency, severity, escalationSuggested)\n",
    "- summary (customerIssue, resolution, followUpRequired)\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(enterprise_prompt)\n",
    "print(\"üìû Call Analysis:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chat"
   },
   "source": [
    "---\n",
    "# Chat Applications\n",
    "\n",
    "## Simple Chat (No Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chat_simple"
   },
   "outputs": [],
   "source": [
    "# Simple chat without memory\n",
    "print(\"üí¨ Simple Chat Demo (type 'exit' to quit)\\n\")\n",
    "\n",
    "# Uncomment to run interactive chat\n",
    "# while True:\n",
    "#     user_input = input(\"You: \")\n",
    "#     if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "#         print(\"Chat ended.\")\n",
    "#         break\n",
    "#     \n",
    "#     response = client.responses.create(\n",
    "#         model=\"gpt-5-nano\",\n",
    "#         input=user_input\n",
    "#     )\n",
    "#     \n",
    "#     print(\"Assistant:\", response.output_text)\n",
    "\n",
    "# Demo example (non-interactive)\n",
    "demo_input = \"What is Maersk's main business?\"\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=demo_input\n",
    ")\n",
    "print(f\"You: {demo_input}\")\n",
    "print(f\"Assistant: {response.output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chat_memory"
   },
   "source": [
    "## Chat with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chat_with_memory"
   },
   "outputs": [],
   "source": [
    "# Chat with conversation history\n",
    "messages = []  # Holds conversation history\n",
    "\n",
    "print(\"üí¨ Chat with Memory Demo\\n\")\n",
    "\n",
    "# Demo conversation\n",
    "demo_messages = [\n",
    "    \"What ports does Maersk operate in India?\",\n",
    "    \"Tell me more about the first port you mentioned.\",\n",
    "    \"What's the container capacity there?\"\n",
    "]\n",
    "\n",
    "for user_input in demo_messages:\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=messages\n",
    "    )\n",
    "    \n",
    "    reply = response.output_text\n",
    "    print(f\"You: {user_input}\")\n",
    "    print(f\"Assistant: {reply}\\n\")\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "print(\"‚úÖ Notice how the assistant remembers context from previous messages!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "function_calling"
   },
   "source": [
    "---\n",
    "# Function Calling & Structured Outputs\n",
    "\n",
    "## Custom Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "function_calling_demo"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define tools schema\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"get_shipment_status\",\n",
    "        \"description\": \"Get the current status and location of a shipment by container number.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"container_number\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The container tracking number (e.g., MAEU1234567)\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"container_number\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "def get_shipment_status(container_number):\n",
    "    return f\"{container_number}: Your container is currently at Mumbai Port, scheduled to depart on 15th October.\"\n",
    "\n",
    "# Create input\n",
    "input_list = [\n",
    "    {\"role\": \"user\", \"content\": \"Where is my shipment? Container number is MAEU1234567.\"}\n",
    "]\n",
    "\n",
    "# Prompt the model with tools\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    input=input_list,\n",
    "    instructions=\"Use Tools if you find it necessary.\"\n",
    ")\n",
    "\n",
    "# Save function call outputs\n",
    "input_list += response.output\n",
    "\n",
    "print(\"üîß Function Calling Demo\")\n",
    "print(\"\\nStep 1: Model decides to call function\")\n",
    "\n",
    "# Execute function if called\n",
    "for item in response.output:\n",
    "    if item.type == \"function_call\":\n",
    "        if item.name == \"get_shipment_status\":\n",
    "            status = get_shipment_status(json.loads(item.arguments))\n",
    "            \n",
    "            input_list.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": item.call_id,\n",
    "                \"output\": json.dumps({\"status\": status})\n",
    "            })\n",
    "            print(f\"Step 2: Function executed: {status}\")\n",
    "\n",
    "# Get final response\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    instructions=\"Respond only with shipment status information generated by a tool.\",\n",
    "    tools=tools,\n",
    "    input=input_list,\n",
    ")\n",
    "\n",
    "print(f\"\\nStep 3: Final response to user:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "structured_output"
   },
   "source": [
    "## Structured Output with Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "structured_output_demo"
   },
   "outputs": [],
   "source": [
    "!pip install pydantic -q\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ContainerBooking(BaseModel):\n",
    "    origin_port: str\n",
    "    destination_port: str\n",
    "    departure_date: str\n",
    "    container_size: str\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the container booking information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I need to book a 40ft container from Mumbai to Rotterdam, departure on 25th October.\"},\n",
    "    ],\n",
    "    text_format=ContainerBooking,\n",
    ")\n",
    "\n",
    "print(\"üìã Structured Output (Pydantic):\")\n",
    "print(response.output_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced_features"
   },
   "source": [
    "---\n",
    "# OpenAI's Advanced Features\n",
    "\n",
    "## GPT-5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpt5_demo"
   },
   "outputs": [],
   "source": [
    "# GPT-5 with Responses API\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=\"Tell me Maersk company inception story in 200 words max\"\n",
    ")\n",
    "\n",
    "print(\"ü§ñ GPT-5 Response:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reasoning"
   },
   "source": [
    "## Reasoning Model (Complex Problem Solving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reasoning_demo"
   },
   "outputs": [],
   "source": [
    "# Reasoning model for complex logistics problem\n",
    "prompt = \"\"\"\n",
    "You are a port operations planner for Jawaharlal Nehru Port (JNPT), Mumbai.\n",
    "There are 250 containers waiting to be loaded onto a vessel. The current loading crane can handle 30 containers per hour.\n",
    "A cyclone warning has been issued, with the storm expected to hit in 7 hours and lasting for 12 hours. If the vessel doesn't depart before the cyclone, port operations will be suspended, causing a delay fee of ‚Çπ8,000 per container.\n",
    "The total delay cost would be ‚Çπ20,00,000 (250 containers √ó ‚Çπ8,000).\n",
    "\n",
    "Option: The port can deploy an additional mobile harbor crane at ‚Çπ1,50,000 per hour, which can load 25 containers per hour.\n",
    "\n",
    "Question:\n",
    "1. Should the port deploy the additional crane or not?\n",
    "2. Show the step-by-step calculation of costs in both scenarios.\n",
    "3. Recommend the best financial option and explain clearly why.\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(\"üßÆ Reasoning Model Analysis:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "audio"
   },
   "source": [
    "## Audio Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "audio_demo"
   },
   "outputs": [],
   "source": [
    "# Audio generation\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=\"Generate audio saying: Your container MAEU1234567 has arrived at Mumbai Port and is ready for pickup.\",\n",
    "    tools=[{\"type\": \"audio_generation\"}]\n",
    ")\n",
    "\n",
    "# Save audio\n",
    "audio_data = [\n",
    "    output.result\n",
    "    for output in response.output\n",
    "    if output.type == \"audio_generation_call\"\n",
    "]\n",
    "\n",
    "if audio_data:\n",
    "    with open(\"shipment_notification.mp3\", \"wb\") as f:\n",
    "        f.write(audio_data[0])\n",
    "    print(\"üîä Audio saved as: shipment_notification.mp3\")\n",
    "    \n",
    "    # Play in Colab\n",
    "    from IPython.display import Audio\n",
    "    display(Audio(\"shipment_notification.mp3\"))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No audio generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "image"
   },
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "image_demo"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image as IPImage\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"Generate an image of a large blue Maersk container ship docked at Mumbai Port during sunset\",\n",
    "    tools=[{\"type\": \"image_generation\"}],\n",
    ")\n",
    "\n",
    "# Save and display image\n",
    "image_data = [\n",
    "    output.result\n",
    "    for output in response.output\n",
    "    if output.type == \"image_generation_call\"\n",
    "]\n",
    "\n",
    "if image_data:\n",
    "    image_base64 = image_data[0]\n",
    "    with open(\"maersk_ship.png\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(image_base64))\n",
    "    print(\"üñºÔ∏è Image saved as: maersk_ship.png\")\n",
    "    display(IPImage(filename=\"maersk_ship.png\"))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No image generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tool_use"
   },
   "source": [
    "---\n",
    "# Tool Use: Web Search & Code Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tool_simple"
   },
   "outputs": [],
   "source": [
    "# Simple Tool Combination\n",
    "print(\"üîß Tool Combination Demo: Web Search + Code Interpreter\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[\n",
    "        {\"type\": \"web_search\"},\n",
    "        {\"type\": \"code_interpreter\", \"container\": {\"type\": \"auto\"}}\n",
    "    ],\n",
    "    input=\"Find the current population of India and calculate how many people that is per square kilometer. India's area is 3.287 million km¬≤. Show your calculation step by step.\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Result:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rag"
   },
   "source": [
    "---\n",
    "# RAG (Retrieval Augmented Generation)\n",
    "\n",
    "## File Search with Vector Store\n",
    "\n",
    "**Note:** To use RAG, you need to:\n",
    "1. Upload a PDF file to Colab\n",
    "2. Update the `PDF_PATH` below\n",
    "3. Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rag_setup"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Upload your PDF file first, then update this path\n",
    "PDF_PATH = Path(\"/content/your_document.pdf\")  # Update this!\n",
    "\n",
    "# Check if file exists\n",
    "if not PDF_PATH.exists():\n",
    "    print(\"‚ö†Ô∏è PDF file not found!\")\n",
    "    print(\"Please upload a PDF file and update the PDF_PATH variable.\")\n",
    "    print(\"\\nTo upload: Click the folder icon on the left ‚Üí Upload file\")\n",
    "else:\n",
    "    print(f\"‚úÖ PDF file found: {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rag_demo"
   },
   "outputs": [],
   "source": [
    "# RAG Demo (only runs if PDF exists)\n",
    "if PDF_PATH.exists():\n",
    "    print(\"\\nüîç Setting up RAG with Vector Store\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create vector store\n",
    "    try:\n",
    "        vector_store = client.beta.vector_stores.create(name=\"maersk_docs_store\")\n",
    "    except AttributeError:\n",
    "        vector_store = client.vector_stores.create(name=\"maersk_docs_store\")\n",
    "    \n",
    "    vector_store_id = vector_store.id\n",
    "    print(f\"‚úÖ Vector store created: {vector_store_id}\")\n",
    "    \n",
    "    # Upload and index PDF\n",
    "    print(\"‚è≥ Uploading & indexing file...\")\n",
    "    with PDF_PATH.open(\"rb\") as f:\n",
    "        try:\n",
    "            client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "                vector_store_id=vector_store_id,\n",
    "                files=[f],\n",
    "            )\n",
    "        except AttributeError:\n",
    "            client.vector_stores.file_batches.upload_and_poll(\n",
    "                vector_store_id=vector_store_id,\n",
    "                files=[f],\n",
    "            )\n",
    "    print(\"‚úÖ File indexed successfully!\\n\")\n",
    "    \n",
    "    # Demo query\n",
    "    demo_question = \"What are the key points in this document?\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=demo_question,\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_id]\n",
    "        }],\n",
    "    )\n",
    "    \n",
    "    print(f\"Question: {demo_question}\")\n",
    "    print(f\"\\nAnswer: {response.output_text}\")\n",
    "    \n",
    "    # Check for citations\n",
    "    if hasattr(response, \"citations\") and response.citations:\n",
    "        print(\"\\nüìö Sources:\")\n",
    "        for c in response.citations:\n",
    "            print(f\"  - {c}\")\n",
    "    else:\n",
    "        print(\"\\n[No citations available]\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping RAG demo - no PDF file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "---\n",
    "# Summary & Next Steps\n",
    "\n",
    "## What You Learned:\n",
    "\n",
    "1. ‚úÖ **Basic Prompting** - Connect to OpenAI API and make simple requests\n",
    "2. ‚úÖ **Advanced Prompting** - Use delimiters, JSON output, conditions, few-shot learning\n",
    "3. ‚úÖ **Chain of Thought** - Break down complex problems step-by-step\n",
    "4. ‚úÖ **Chat Applications** - Build chatbots with and without memory\n",
    "5. ‚úÖ **Function Calling** - Integrate custom tools and functions\n",
    "6. ‚úÖ **Structured Outputs** - Use Pydantic for type-safe responses\n",
    "7. ‚úÖ **Advanced Features** - Audio, image generation, reasoning models\n",
    "8. ‚úÖ **Tool Use** - Web search, code interpreter\n",
    "9. ‚úÖ **RAG** - Build document Q&A with vector stores\n",
    "\n",
    "## Resources:\n",
    "\n",
    "- [OpenAI Documentation](https://platform.openai.com/docs)\n",
    "- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! üö¢**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
